# Numbers to remember:


| **Component**      | **Type / Spec**                       | **Throughput / Capacity**                | **Latency**   | ⚠️ **When to Scale / Shard**                                                                   | 📌 **Scaling Tips**                                                        |
|--------------------|----------------------------------------|------------------------------------------|---------------|------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| **App Server**     | 8-core CPU, 32 GB RAM                 | 10k–50k req/sec                          | 1–10 ms       | 💡 CPU > 70%, queue latency rises, threads blocking                                            | ➕ Add more instances (stateless), use async, batch I/O                     |
| **Load Balancer**  | AWS ALB / NGINX                       | 100k+ req/sec                            | <1 ms         | 💡 CPU > 50%, dropped connections, high queueing                                               | ➕ Add more nodes, layer 4 LB (e.g., TCP) for ultra-high throughput         |
| **API Gateway**    | Kong / API Gateway                    | 10k–20k req/sec                          | 5–20 ms       | 💡 Latency spikes on auth/rate limit, too many throttled calls                                 | ➕ Deploy gateway per service group, enable caching                        |
| **SQL DB**         | PostgreSQL, MySQL                     | 2k–10k TPS                               | 1–20 ms       | 💡 Connections > max, locks/waits increase, query latency > 100 ms                             | ➕ Read replicas, DB partitioning, index tuning                            |
| **NoSQL DB**       | MongoDB, Cassandra, DynamoDB          | 10k–100k TPS                             | 5–15 ms       | 💡 Partition imbalance, hot key patterns, write timeouts                                       | ➕ Shard by access pattern (e.g., userID), tune consistency levels         |
| **NewSQL DB**      | CockroachDB, Yugabyte                 | 5k–50k TPS                               | 5–20 ms       | 💡 Slow multi-region writes, coordination overhead                                             | ➕ Geo-partition by region, use follower reads                             |
| **Redis (Cache)**  | 32 GB RAM                             | ~100k ops/sec                            | <1 ms         | 💡 Evictions increase, cache hit rate < 80%, CPU > 70%                                         | ➕ Add replicas, cluster Redis (partition keys), LRU tuning                |
| **Memcached**      | 64 GB RAM                             | 200k+ ops/sec                            | <1 ms         | 💡 Hit rate drops, memory full, inconsistent TTL                                               | ➕ Consistent hashing across nodes, auto-evict stale keys                  |
| **CDN**            | Cloudflare, Fastly                    | Tbps+                                    | 20–100 ms     | 💡 Cache hit ratio < 90%, rising origin load                                                   | ➕ Tune TTLs, edge rules, use origin shielding                             |
| **Object Store**   | S3, GCS                               | ~3500 PUT/sec, 5500 GET/sec per prefix  | 50–200 ms     | 💡 503 errors, prefix contention, slow batch jobs                                              | ➕ Use random prefixes, multipart uploads, batch reads                     |
| **File System**    | EBS, SSD FS                           | 500–10k IOPS                             | 0.1–5 ms      | 💡 IO latency spikes, DB fsync slow                                                            | ➕ Use provisioned IOPS, move to local NVMe                                |
| **Message Queue**  | Kafka, SQS, RabbitMQ                  | Kafka: 1M msg/sec/cluster                | 1–100 ms      | 💡 Lag growing, consumer backlog, disk usage > 80%                                             | ➕ Add partitions, consumer groups, disk-based compaction                  |
| **Search Engine**  | Elasticsearch, OpenSearch             | 5k–20k QPS/node                          | 10–100 ms     | 💡 High heap usage, GC thrashing, slow indexing                                                | ➕ Add shards, split indices (e.g., time-based), async bulk indexing       |
| **Analytics DB**   | ClickHouse, Druid, BigQuery           | 100k+ rows/sec ingest                    | ~100 ms–5s    | 💡 Query latency > 3s, ingest lag, memory full                                                 | ➕ Horizontal scale, pre-aggregated tables, query materialization          |
| **Monitoring**     | Prometheus, Grafana                   | 10k–100k series/sec                      | --            | 💡 TSDB size bloats, scrape lag > 30s                                                          | ➕ Use remote write, label cardinality control                             |
| **Notification**   | Firebase, OneSignal                   | ~10k–100k/min                            | --            | 💡 Throttling errors, poor delivery rates                                                      | ➕ Segment campaigns, use delivery windows                                 |
| **Email Service**  | SES, SendGrid                         | SES: 14k/sec max                         | 1–5 sec       | 💡 Bounce rates rise, send errors, blacklist warnings                                          | ➕ Warm-up IPs, use dedicated pools                                        |
