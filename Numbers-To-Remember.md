# Numbers to remember:


| **Component**      | **Type / Spec**                       | **Throughput / Capacity**                | **Latency**   | âš ï¸ **When to Scale / Shard**                                                                   | ðŸ“Œ **Scaling Tips**                                                        |
|--------------------|----------------------------------------|------------------------------------------|---------------|------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| **App Server**     | 8-core CPU, 32 GB RAM                 | 10kâ€“50k req/sec                          | 1â€“10 ms       | ðŸ’¡ CPU > 70%, queue latency rises, threads blocking                                            | âž• Add more instances (stateless), use async, batch I/O                     |
| **Load Balancer**  | AWS ALB / NGINX                       | 100k+ req/sec                            | <1 ms         | ðŸ’¡ CPU > 50%, dropped connections, high queueing                                               | âž• Add more nodes, layer 4 LB (e.g., TCP) for ultra-high throughput         |
| **API Gateway**    | Kong / API Gateway                    | 10kâ€“20k req/sec                          | 5â€“20 ms       | ðŸ’¡ Latency spikes on auth/rate limit, too many throttled calls                                 | âž• Deploy gateway per service group, enable caching                        |
| **SQL DB**         | PostgreSQL, MySQL                     | 2kâ€“10k TPS                               | 1â€“20 ms       | ðŸ’¡ Connections > max, locks/waits increase, query latency > 100 ms                             | âž• Read replicas, DB partitioning, index tuning                            |
| **NoSQL DB**       | MongoDB, Cassandra, DynamoDB          | 10kâ€“100k TPS                             | 5â€“15 ms       | ðŸ’¡ Partition imbalance, hot key patterns, write timeouts                                       | âž• Shard by access pattern (e.g., userID), tune consistency levels         |
| **NewSQL DB**      | CockroachDB, Yugabyte                 | 5kâ€“50k TPS                               | 5â€“20 ms       | ðŸ’¡ Slow multi-region writes, coordination overhead                                             | âž• Geo-partition by region, use follower reads                             |
| **Redis (Cache)**  | 32 GB RAM                             | ~100k ops/sec                            | <1 ms         | ðŸ’¡ Evictions increase, cache hit rate < 80%, CPU > 70%                                         | âž• Add replicas, cluster Redis (partition keys), LRU tuning                |
| **Memcached**      | 64 GB RAM                             | 200k+ ops/sec                            | <1 ms         | ðŸ’¡ Hit rate drops, memory full, inconsistent TTL                                               | âž• Consistent hashing across nodes, auto-evict stale keys                  |
| **CDN**            | Cloudflare, Fastly                    | Tbps+                                    | 20â€“100 ms     | ðŸ’¡ Cache hit ratio < 90%, rising origin load                                                   | âž• Tune TTLs, edge rules, use origin shielding                             |
| **Object Store**   | S3, GCS                               | ~3500 PUT/sec, 5500 GET/sec per prefix  | 50â€“200 ms     | ðŸ’¡ 503 errors, prefix contention, slow batch jobs                                              | âž• Use random prefixes, multipart uploads, batch reads                     |
| **File System**    | EBS, SSD FS                           | 500â€“10k IOPS                             | 0.1â€“5 ms      | ðŸ’¡ IO latency spikes, DB fsync slow                                                            | âž• Use provisioned IOPS, move to local NVMe                                |
| **Message Queue**  | Kafka, SQS, RabbitMQ                  | Kafka: 1M msg/sec/cluster                | 1â€“100 ms      | ðŸ’¡ Lag growing, consumer backlog, disk usage > 80%                                             | âž• Add partitions, consumer groups, disk-based compaction                  |
| **Search Engine**  | Elasticsearch, OpenSearch             | 5kâ€“20k QPS/node                          | 10â€“100 ms     | ðŸ’¡ High heap usage, GC thrashing, slow indexing                                                | âž• Add shards, split indices (e.g., time-based), async bulk indexing       |
| **Analytics DB**   | ClickHouse, Druid, BigQuery           | 100k+ rows/sec ingest                    | ~100 msâ€“5s    | ðŸ’¡ Query latency > 3s, ingest lag, memory full                                                 | âž• Horizontal scale, pre-aggregated tables, query materialization          |
| **Monitoring**     | Prometheus, Grafana                   | 10kâ€“100k series/sec                      | --            | ðŸ’¡ TSDB size bloats, scrape lag > 30s                                                          | âž• Use remote write, label cardinality control                             |
| **Notification**   | Firebase, OneSignal                   | ~10kâ€“100k/min                            | --            | ðŸ’¡ Throttling errors, poor delivery rates                                                      | âž• Segment campaigns, use delivery windows                                 |
| **Email Service**  | SES, SendGrid                         | SES: 14k/sec max                         | 1â€“5 sec       | ðŸ’¡ Bounce rates rise, send errors, blacklist warnings                                          | âž• Warm-up IPs, use dedicated pools                                        |
